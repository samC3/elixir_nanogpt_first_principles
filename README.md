# Elixir NanoGPT: Learning LLMs from first principles

This collection of Livebooks are tracking my progress of understanding [Andrej Karpathy's neural networks video series](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ). After watching through the series and following along with the python examples I found I wasn't properly building an understanding of the material. To make sure I understood the concepts I've decided to rebuild the apps in Elixir from a functional programming approach. This means my solutions will vary slightly from those in the videos, which I will do my best to explain along the way.

I've extended on the content in the videos with other series and books to ensure I understand how neural networks from first principles. One example of this is diving into calculus to gain a better understanding of backpropagation. I've listed the material I used with a brief summary in the Livebooks where relevant.

### Progress

- [x] Micrograd
- [x] Makemore (part 1 & 2)
- [ ] Makemore (part 3 & 4)
- [ ] Wavenet
- [ ] GPT
- [ ] Tokeniser
- [ ] GPT-2
- [ ] nanoGPT
- [ ] nanochat