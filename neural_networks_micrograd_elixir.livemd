<!-- livebook:{"file_entries":[{"name":"gradient.gif","type":"attachment"},{"name":"training_demo_1.gif","type":"attachment"}],"persist_outputs":true} -->

# Neural networks from first principles, Micrograd Elixir

```elixir
Mix.install([
  {:kino, "~> 0.17.0"},
  {:tucan, "~> 0.5.0"},
  {:kino_vega_lite, "~> 0.1.13"}
])
```

## Introduction

This livebook is a recreation of [Andrej Karpathy's Micrograd](https://www.youtube.com/watch?v=VMj-3S1tku0) rewritten using functional programming and the Elixir programming language. The goal is to build up knowledge about neural networks from first principles before moving into the more complicated architecutres of how large language models works.

## What is being built

Neural networks essentially boil down to a mathematical calculation with a huge number of values that are adjusted based on the training data provided. Once trained, new values similar to the training data are passed as input and are run through the same calculation to produce the networks answer for what it determines is approximately correct.

As a contrived example, the result of this function can be approximated by giving a small network the input $x$ and result values to learn from.

$$
sin(4x) * cos(5x)
$$

This illustration shows where the network begins randomly guessing values for $x$ but over time improves it's answer and gets closer and closer to the actual result. (Note: this example is "overfitting" the training data, which is something to be avoided in a real example).

<img src="files/training_demo_1.gif" alt="A small neural network being trained to approximate the sin(4x) * cos(5x) function" width="500" />

There are all kinds of libraries (e.g. TensorFlow, PyTorch, Nx, Axon) that provide the tools to build and scale these networks. But to understand them from first principles to gain an intuitive sense of what is going on, tensors and linear algebra are not required right away. A functional neural network can be built using just straight foward Elixir.

### A functional approach

This Livebook departs in the implementation to demostrate how a neural network can be expressed with a functional language, and how it can result in code that is easier to reason about since the pure functions are explicit at every step along the way.

## Step 1: A computational graph

The great news is the mathematical concepts and formulas needed to understand can be built up from high school level math. Basic arthimatic and certain parts of calculus are the fundamental building blocks of a neural network.

Neural network can be thought of as a computational graph which takes a list of numbers and handles the various mathematical operations to produce a list of numbers as an output.

For example, to show $2 \times 3 = 6$ as a simple computational graph it would look like this, where it takes in a list containing `2` and `3` as inputs, and produces the output of `6`.

```mermaid
---
config:
  look: handDrawn
  theme: forest
---
flowchart LR
2 --> multiply
3 --> multiply
multiply --> 6
```

Elixir can complete mathematical operations in the language with simple code built into the main library.

```elixir
2 * 3
```

<!-- livebook:{"output":true} -->

```
6
```

Instead of working with raw numbers (integers and floats), the values will be wrapped in a `%Value{}` [struct](https://hexdocs.pm/elixir/structs.html). This will allow them to store the intermediary values to keep track of the calculation being run.

The `Value` will hold information about a number in the computational graph:

* `data`: the underlying value
* `op`: which mathematical operation created the `Value`. May be `nil` for inputs
* `children`: the `Value` instances used to create this value
* `label`: used to store the name of the variable to be rendered in the Mermaid diagram
* `id`: used when creating the Mermaid diagram

A struct is defined with the `defstruct` construct which lists the fields. All that is really _needed_ to define is:

<!-- livebook:{"force_markdown":true} -->

```elixir
defmodule Value do
  defstruct [:data, :op, :children, :label, :id]
end
```

However, the implementation does not want to be concerned about creating the value for `id` for the diagrams as more complexity is defined in the livebook so a `new/2` function is included.

It should be noted that Andrej Karpathy's python version incluides a `grad` field on the `Value` class. This has been intentionally left out since this version is taking a slightly different implementation with functional programming, which will be explained in more detail further along.

```elixir
defmodule Value do
  defstruct [:data, :op, :label, :id, children: []]

  def new(data, label \\ "", children \\ [], op \\ nil) do
    %__MODULE__{
      data: data,
      children: children,
      op: op,
      label: label,
      id: :erlang.unique_integer([:monotonic, :positive])
    }
  end
end

Value.new(2.0, "x")
```

<!-- livebook:{"output":true} -->

```
%Value{data: 2.0, op: nil, label: "x", id: 1, children: []}
```

Given this data structure a computational graph can be made to match the diagram above:

```elixir
two = Value.new(2.0, "two")
three = Value.new(3.0, "three")

Value.new(two.data * three.data, "six", [two, three])
```

<!-- livebook:{"output":true} -->

```
%Value{
  data: 6.0,
  op: nil,
  label: "six",
  id: 4,
  children: [
    %Value{data: 2.0, op: nil, label: "two", id: 2, children: []},
    %Value{data: 3.0, op: nil, label: "three", id: 3, children: []}
  ]
}
```

Since the underlying mathematical functions will not work with the `Value` data structure an additional module is required perform operations on the values so mathimatical functions can be built with this new data model.

The `Math` module implements the operations required to construct new `Value` structures from existing values. The `data` field is created using the usual Elixir/Erlang functions to calculate the values and the rest of the function is building up the metadata to be used in inspecting the computational graph.

```elixir
defmodule Math do
  def add(value_one, value_two, label) do
    Value.new(
      value_one.data + value_two.data,
      label,
      [value_one, value_two],
      :add
    )
  end

  def mul(value_one, value_two, label) do
    Value.new(
      value_one.data * value_two.data,
      label,
      [value_one, value_two],
      :mul
    )
  end

  def sum(values, label) do
    Value.new(
      Enum.reduce(values, 0.0, & &1.data + &2),
      label,
      values,
      :sum
    )
  end

  def pow(value, exp, label) do
    Value.new(
      :math.pow(value.data, exp.data),
      label,
      [value, exp],
      :pow
    )
  end

  def tanh(value, label) do
    Value.new(
      :math.tanh(value.data),
      label,
      [value],
      :tanh
    )
  end

  def relu(value, label) do
    Value.new(
      Enum.max([value.data, 0]),
      label,
      [value],
      :relu
    )
  end
end

Math.mul(two, three, "six")
```

<!-- livebook:{"output":true} -->

```
%Value{
  data: 6.0,
  op: :mul,
  label: "six",
  id: 5,
  children: [
    %Value{data: 2.0, op: nil, label: "two", id: 2, children: []},
    %Value{data: 3.0, op: nil, label: "three", id: 3, children: []}
  ]
}
```

With the `Math` module mathematical expressions can be created which can be used to view the computational graphs. An additional module is required produce programmtically these mermaid diagrams with the values of each filled in, and further along the gradients as well.

```mermaid
---
config:
  look: handDrawn
  theme: natural
---
flowchart LR
x --> multiply
y --> multiply
multiply --> add
z --> add
add --> result
```

```elixir
x = Value.new(2.0, "x")
y = Value.new(3.0, "y")
z = Value.new(4.0, "z")

result = Math.mul(x, y, "x*y") |> Math.add(z, "result")
```

<!-- livebook:{"output":true} -->

```
%Value{
  data: 10.0,
  op: :add,
  label: "result",
  id: 10,
  children: [
    %Value{
      data: 6.0,
      op: :mul,
      label: "x*y",
      id: 9,
      children: [
        %Value{data: 2.0, op: nil, label: "x", id: 6, children: []},
        %Value{data: 3.0, op: nil, label: "y", id: 7, children: []}
      ]
    },
    %Value{data: 4.0, op: nil, label: "z", id: 8, children: []}
  ]
}
```

This Value struct shows the final result of the $x \times y + z$ function is `10`, storing all of the intermediate values along the way right back to the intital values.

The `Mermaid` module provides a function to visualize the computational graph in an easier way.

Note: it's not essential to understand this module so feel free to skip over. Also worth noting that this approach will stop working once the computational graphs become sufficiently complicated, but at that stage a full graphic may begin to lose its value.

```elixir
defmodule Mermaid do
  def render_value(value, grad_map \\ %{}) do
    """
    ---
    config:
      theme: base
    ---
    flowchart LR
    #{to_mermaid(value, grad_map)}
    """
    |> Kino.Mermaid.new()
  end

  defp to_mermaid(nil, _grad_map), do: ""

  defp to_mermaid(%{children: []} = value, grad_map) do
    value_node(value, grad_map)
  end

  defp to_mermaid(%{children: children, op: op} = value, grad_map) do
    children_ids = Enum.reduce(children, "", & &2 <> to_string(&1.id))
    operator_node_id = "#{children_ids}#{op}"

    """
    #{
      for child <- children, reduce: "" do
        acc ->
         acc <> "\n #{to_mermaid(child, grad_map)} --> #{operator_node_id}[#{op}]"
      end
    }
    #{operator_node_id}[#{op}] --> #{value_node(value, grad_map)}
    """
  end

  defp value_node(%{id: id, data: data, label: label}, grad_map) do
    """
    #{id}[#{label}
    data: #{data}
    grad: #{Map.get(grad_map, id, 0.0)}]
    """
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, Mermaid, <<70, 79, 82, 49, 0, 0, 18, ...>>, {:value_node, 2}}
```

The `grad` field will be populated in the next section.

```elixir
Mermaid.render_value(result)
```

<!-- livebook:{"output":true} -->

```mermaid
---
config:
  theme: base
---
flowchart LR

 
 6[x
data: 2.0
grad: 0.0]
 --> 67mul[mul]
 7[y
data: 3.0
grad: 0.0]
 --> 67mul[mul]
67mul[mul] --> 9[x*y
data: 6.0
grad: 0.0]

 --> 98add[add]
 8[z
data: 4.0
grad: 0.0]
 --> 98add[add]
98add[add] --> 10[result
data: 10.0
grad: 0.0]



```

The computational graph shows the calculation as values are passed forwards through the calculation, but in order to train a neural network there needs to be a mechanism to traverse backwards through the calculation and determine the impact of each individual value so that training can occur.

A brief step into calculus is needed to continue the journey of first principles understanding before working through backpropagation and training.

## Calculus

Note: Two useful sources for building an understanding of calculus are:

1. **The essence of calculus** series on youtube by 3Blue1Brown, and
2. **Calculus Made Easy** by Silvanus P. Thompson, first published in 1910 (free pdf copies can be found online)

This quote from *Calculus made easy* explains the first part of what is needed from calculus:

> $d$ which merely means “a little bit of.”
> Thus $dx$ means a little bit of $x$; or $du$ means a little bit of $u$. Ordinary mathematicians think it more polite to say “an element of,”
> instead of “a little bit of.” Just as you please. But you will find that
> these little bits (or elements) may be considered to be indefinitely small.

This is need with the calculations created here, observing what happens when the values are nudged by just a little bit. The second part is highlighted in this quote:

> Moreover, we are usually dealing with more than one variable at once, and thinking of the way in which one variable depends on the other: ... Or we are asked to consider a rectangle of given area, and to enquire how any increase in the length of it will compel a corresponding decrease in the breadth of it. Or we think of the way in which any variation in the slope of a ladder will cause the height that it reaches, to vary.
> 
> Suppose we have got two such variables that depend one on the other. An alteration in one will bring about an alteration in the other, because of this dependence.

Given the computational graphs with multiple variable that are dependent on each other, if $x$ increases in value, then, with the small parts to be measured that change the occurs to the overall equation. (A detour to the essence of calculus is highly recommended).

When building neural networks it isn't realistic to go through the full process of deriving the derivative since it would be an expression with tens of thousands of terms. Instead, an understanding of that the derivative is, and why it's useful in the context of building these networks.

### A single variable function

Starting with a simple parabola $y = x^2$, what happens to $y$ if $x$ is adjusted by just a tiny amount?

First, define the function `fn x -> x**2 end` that can be used to calculate the value for `y`

```elixir
f = fn x -> x**2 end

_y = f.(3)
```

<!-- livebook:{"output":true} -->

```
9
```

And with this function calculate the values for $y$ when $x$ is between -10 and 10:

```elixir
xs =
  Enum.map(
    -10..10,
    & %{x: &1, y: f.(&1), type: "curve"}
  )

Tucan.lineplot(xs, "x", "y")
```

<!-- livebook:{"output":true} -->

```vega-lite
{"$schema":"https://vega.github.io/schema/vega-lite/v5.json","__tucan__":{"types":{"type":"nominal","x":"quantitative","y":"quantitative"}},"data":{"values":[{"type":"curve","x":-10,"y":100},{"type":"curve","x":-9,"y":81},{"type":"curve","x":-8,"y":64},{"type":"curve","x":-7,"y":49},{"type":"curve","x":-6,"y":36},{"type":"curve","x":-5,"y":25},{"type":"curve","x":-4,"y":16},{"type":"curve","x":-3,"y":9},{"type":"curve","x":-2,"y":4},{"type":"curve","x":-1,"y":1},{"type":"curve","x":0,"y":0},{"type":"curve","x":1,"y":1},{"type":"curve","x":2,"y":4},{"type":"curve","x":3,"y":9},{"type":"curve","x":4,"y":16},{"type":"curve","x":5,"y":25},{"type":"curve","x":6,"y":36},{"type":"curve","x":7,"y":49},{"type":"curve","x":8,"y":64},{"type":"curve","x":9,"y":81},{"type":"curve","x":10,"y":100}]},"encoding":{"x":{"field":"x","type":"quantitative"},"y":{"field":"y","type":"quantitative"}},"mark":{"fillOpacity":1,"type":"line"}}
```

The derivative can be found by looking at the gradient of the slope an any point along this graph. This is commonly referred to as the _"instantanous rate of change"_, but as Grant points out in the Essence of Calculus, this is a paradox. Can something change when looking at it as a snapshot in single moment of time?

It's more helpful to think of this is how much does it change by between two very very close points in time. Or in this example, how much it change if $x$ is made just a tiny little bit bigger, say by `0.000001`.

That is is what this defintion form wikipedia is saying, as $h$ gets closer and closer to zero what is the impact on the output.

$$
\lim_{h \to 0} \frac{f(x + h) - f(x)}{h}
$$

This can be written out as:

```elixir
h = 0.000001
x = 3

(f.(x + h) - f.(x)) / h
```

<!-- livebook:{"output":true} -->

```
6.000001000927568
```

This shows that when $x = 3$ the _"rate of change"_, or _"gradient"_ of the function when the value is nudged just a little bit it approximately $6$.

Putting that formula into a function and then looking at different values of $x$ shows how the gradient changes for large and smaller values.

```elixir
calc_grad = fn x, h -> (f.(x + h) - f.(x)) / h end

tangent_line =
  fn x, h ->
    Enum.map(-2..2, fn i ->
      grad = calc_grad.(x, h)
      %{x: x + i, y: f.(x) + grad * ((x + i) - x), type: "tangent"}
    end)
  end

Enum.map([2, 0.4, -3], fn x ->
  Tucan.lineplot(
    xs ++ tangent_line.(x, h),
    "x",
    "y",
    color_by: "type",
    title: "grad is #{Float.round(calc_grad.(x, h), 1)} when x is #{x}"
  )
end)
|> Tucan.hconcat()
```

<!-- livebook:{"output":true} -->

```vega-lite
{"$schema":"https://vega.github.io/schema/vega-lite/v5.json","hconcat":[{"__tucan__":{"types":{"type":"nominal","x":"quantitative","y":"quantitative"}},"data":{"values":[{"type":"curve","x":-10,"y":100},{"type":"curve","x":-9,"y":81},{"type":"curve","x":-8,"y":64},{"type":"curve","x":-7,"y":49},{"type":"curve","x":-6,"y":36},{"type":"curve","x":-5,"y":25},{"type":"curve","x":-4,"y":16},{"type":"curve","x":-3,"y":9},{"type":"curve","x":-2,"y":4},{"type":"curve","x":-1,"y":1},{"type":"curve","x":0,"y":0},{"type":"curve","x":1,"y":1},{"type":"curve","x":2,"y":4},{"type":"curve","x":3,"y":9},{"type":"curve","x":4,"y":16},{"type":"curve","x":5,"y":25},{"type":"curve","x":6,"y":36},{"type":"curve","x":7,"y":49},{"type":"curve","x":8,"y":64},{"type":"curve","x":9,"y":81},{"type":"curve","x":10,"y":100},{"type":"tangent","x":0,"y":-4.000002001296025},{"type":"tangent","x":1,"y":-1.000648012450256e-6},{"type":"tangent","x":2,"y":4.0},{"type":"tangent","x":3,"y":8.000001000648012},{"type":"tangent","x":4,"y":12.000002001296025}]},"encoding":{"color":{"field":"type"},"x":{"field":"x","type":"quantitative"},"y":{"field":"y","type":"quantitative"}},"mark":{"fillOpacity":1,"type":"line"},"title":"grad is 4.0 when x is 2"},{"__tucan__":{"types":{"type":"nominal","x":"quantitative","y":"quantitative"}},"data":{"values":[{"type":"curve","x":-10,"y":100},{"type":"curve","x":-9,"y":81},{"type":"curve","x":-8,"y":64},{"type":"curve","x":-7,"y":49},{"type":"curve","x":-6,"y":36},{"type":"curve","x":-5,"y":25},{"type":"curve","x":-4,"y":16},{"type":"curve","x":-3,"y":9},{"type":"curve","x":-2,"y":4},{"type":"curve","x":-1,"y":1},{"type":"curve","x":0,"y":0},{"type":"curve","x":1,"y":1},{"type":"curve","x":2,"y":4},{"type":"curve","x":3,"y":9},{"type":"curve","x":4,"y":16},{"type":"curve","x":5,"y":25},{"type":"curve","x":6,"y":36},{"type":"curve","x":7,"y":49},{"type":"curve","x":8,"y":64},{"type":"curve","x":9,"y":81},{"type":"curve","x":10,"y":100},{"type":"tangent","x":-1.6,"y":-1.4400019999462543},{"type":"tangent","x":-0.6,"y":-0.6400009999731272},{"type":"tangent","x":0.4,"y":0.16000000000000003},{"type":"tangent","x":1.4,"y":0.9600009999731272},{"type":"tangent","x":2.4,"y":1.7600019999462546}]},"encoding":{"color":{"field":"type"},"x":{"field":"x","type":"quantitative"},"y":{"field":"y","type":"quantitative"}},"mark":{"fillOpacity":1,"type":"line"},"title":"grad is 0.8 when x is 0.4"},{"__tucan__":{"types":{"type":"nominal","x":"quantitative","y":"quantitative"}},"data":{"values":[{"type":"curve","x":-10,"y":100},{"type":"curve","x":-9,"y":81},{"type":"curve","x":-8,"y":64},{"type":"curve","x":-7,"y":49},{"type":"curve","x":-6,"y":36},{"type":"curve","x":-5,"y":25},{"type":"curve","x":-4,"y":16},{"type":"curve","x":-3,"y":9},{"type":"curve","x":-2,"y":4},{"type":"curve","x":-1,"y":1},{"type":"curve","x":0,"y":0},{"type":"curve","x":1,"y":1},{"type":"curve","x":2,"y":4},{"type":"curve","x":3,"y":9},{"type":"curve","x":4,"y":16},{"type":"curve","x":5,"y":25},{"type":"curve","x":6,"y":36},{"type":"curve","x":7,"y":49},{"type":"curve","x":8,"y":64},{"type":"curve","x":9,"y":81},{"type":"curve","x":10,"y":100},{"type":"tangent","x":-5,"y":20.999998001499534},{"type":"tangent","x":-4,"y":14.999999000749767},{"type":"tangent","x":-3,"y":9.0},{"type":"tangent","x":-2,"y":3.000000999250233},{"type":"tangent","x":-1,"y":-2.9999980014995344}]},"encoding":{"color":{"field":"type"},"x":{"field":"x","type":"quantitative"},"y":{"field":"y","type":"quantitative"}},"mark":{"fillOpacity":1,"type":"line"},"title":"grad is -6.0 when x is -3"}]}
```

This renders the gradient as a slope on the parabola to visualize the "steepness", i.e. the higher rate of change as $x$ grows larger. Notice that there is a negative slope when $x$ is a negative value.

<!-- livebook:{"break_markdown":true} -->

### Multi variable functions

Consider a calcualtion that takes three variables instead of one, e.g. $x \times y + z$. The aim is to know what happens when $x$, $y$ or $z$ are adjusted just a small amount and how that impacts the result.

A function can be created to calculate the result:

```elixir
x = 8
y = 10
z = 4

f = fn x, y, z -> x * y + z end

f.(x, y, z)
```

<!-- livebook:{"output":true} -->

```
84
```

Each variable can now be increased separately using the same approach above to see the resulting gradient.

```elixir
h = 0.000000001

x_grad = (f.(x + h, y, z) - f.(x, y, z)) / h
y_grad = (f.(x, y + h, z) - f.(x, y, z)) / h
z_grad = (f.(x, y, z + h) - f.(x, y, z)) / h

IO.write """
gradient of x: #{x_grad}
gradient of y: #{y_grad}
gradient of z: #{z_grad}
"""
```

<!-- livebook:{"output":true} -->

```
gradient of x: 10.000007932831068
gradient of y: 8.000000661922968
gradient of z: 1.0000036354540498
```

<!-- livebook:{"output":true} -->

```
:ok
```

Some interesting observations about the results of these gradients:

1. The gradient of $z$ is always close to 1 no matter the value
2. The gradient of $x$ is approximately the value $y$, and vice-versa

#### Addition rule

This result can be proved algebraically. Take this simple equation where the goal is to find the derivative of $c$ with respect to $b$, usually written as $\frac{dc}{db}$

$$
c = a + b
$$

A small amount called $db$ is added to $b$ and the same is done for $c$ (called $dc$).

$$
c + dc = a + b + db 
$$

The goal is to find the value $\frac{dc}{db}$ so the unnesseary values need to be removed. Since $c$ is equal to $a + b$ it is possible to subtract $c$ from the left hand side and $a + b$ from the right hand side to simplify the calculation, which leaves:

$$
dc = db
$$

Dividing both sides by $db$ arrives at the answer for $\frac{dc}{db}$, which will be one given $db$ divided by itself is equal to one

$$
\frac{dc}{db} = 1
$$

#### Product rule

If the same equation is multiplied instead it can be proved how the gradient of one variable will be the value of the other. To solve for $\frac{dc}{db}$ for this equation:

$$
c = a \times b
$$

The small amounts for $b$ and $c$ are added

$$
c + dc = a \times b + db
$$

Again, subtracting the original $c = a \times b$ from each side leaves:

$$
dc = a \times db
$$

Dividing each side by $db$ results in:

$$
\frac{dc}{db} = a
$$

Repeating the process by solving for $\frac{dc}{da}$ would find that returns $b$ as the result, proving the product rule.

#### Power rule

The derivative for an exponent can be proved following the same process algebraically. Starting with:

$$
b = a^2
$$

A small amount is added to $a$ and the correseponding change to $b$

$$
b + db = (a + da)^2
$$

Expanding this out looks at the sqaure of the $a + da$ as it multiplying by itself.

$$
b + db = (a + da) \times (a + da)
$$

Which can be expanded with the binomial therom:

$$
b + db = a^2 + (2 \times a \times da) + da^2
$$

At this point it is reasonable to ignore $da^2$ since a small, almost zero number multiplied by itself becomes an even smaller number. So its impact on the overall equation is negligable.

$$
b + db = a^2 + (2 \times a \times da)
$$

Subtract the original equation

$$
db = 2 \times a \times da
$$

Finally divide both sides by $da$ to get the result of $\frac{dc}{da}$

$$
\frac{db}{da} = 2a
$$

Repeating the same process for $b = a^3$ would find the result to $\frac{dc}{da} = 3a^2$.

The pattern here is the derivative of $a^n$ will be $na^{n-1}$

<!-- livebook:{"break_markdown":true} -->

#### Chain rule

_Calculus Made Easy_ describes the chain rule as "a useful dodge" which is perfect for this use case. It allows the rate of change between different values in a complicated function to be found easily.

The chain rule is simply this:

$$
\frac{dz}{dx} = \frac{dz}{dy} . \frac{dy}{dx}
$$

Wikipedia has a useful explanation:

> In calculus, the chain rule is a formula that expresses the derivative of the composition of two differentiable functions f and g in terms of the derivatives of f and g.
> 
> ...
> 
> Intuitively, the chain rule states that knowing the instantaneous rate of change of z relative to y and that of y relative to x allows one to calculate the instantaneous rate of change of z relative to x as the product of the two rates of change.
> 
> If a car travels twice as fast as a bicycle and the bicycle is four times as fast as a walking man, then the car travels 2 × 4 = 8 times as fast as the man.

[Wikipedia Chain Rule](https://en.wikipedia.org/wiki/Chain_rule)

Programmatically, the equation $x \times y + z$ could also be written as: `Math.add(Math.mul(x, y), z)`.

<!-- livebook:{"break_markdown":true} -->

With this knowledge a `Grad` module can now be created that takes in a `Value` and determines the gradients of it's children, i.e. the values that created the one passed in.

For example with this simplifed value:

<!-- livebook:{"force_markdown":true} -->

```elixir
%Value{
  id: 3,
  op: :mul,
  data: 50,
  children:[
    %Value{data: 5, id: 1},
    %Value{data: 10, id: 2}
  ]
}
```

The `Grad` module can automatically determine the gradients of each child and return them with their IDs:

<!-- livebook:{"force_markdown":true} -->

```elixir
[{1, 10}, {2, 5}]
```

Note: the use of the term children here can be confusing since really, these are the parent values that created the Value being passed in. Andrej used the term children since the final value of the calculation is effectively the root node in the graph, and the previous values branch away as its child nodes. The name is left the same here to make it easier to compare back to the original micrograd library.

```elixir
defmodule Grad do
  # As shown above the gradient from addition results in 1. Then given the
  # chain rule the gradients coming from the parent are multiplied to
  # become value_grad * 1. This just passes through the gradient
  # from the upstream value to the children.
  def calculate(%Value{op: :add, children: [left, right]}, value_grad) do
    [
      {left.id, value_grad},
      {right.id, value_grad}
    ]
  end

  # The same as above but passing to all of the children
  def calculate(%Value{op: :sum, children: children}, value_grad) do
    Enum.map(children, & {&1.id, value_grad})
  end

  # As the product rule above proved the gradient of each child is the 
  # value of the other child. Then for the chain rule this is multiplied by
  # the value gradient to get the final gradient value for each child.
  def calculate(%Value{op: :mul, children: [left, right]}, value_grad) do
    [
      {left.id, right.data * value_grad},
      {right.id, left.data * value_grad}
    ]
  end

  # This is the power rule described above then multiplied by the value_grad
  def calculate(%Value{op: :pow, children: [child, exp]}, value_grad) do
    [
      {child.id, (exp.data * child.data ** (exp.data - 1)) * value_grad}
    ]
  end

  # For the sake of brevity the quotient rule and the working
  # for tanh are omitted, but wikipedia has a good write to explain the process.
  def calculate(%Value{op: :tanh, children: [child], data: data}, value_grad) do
    [
      {child.id, (1 - data ** 2) * value_grad}
    ]
  end

  def calculate(%Value{op: :relu, children: [child], data: data}, value_grad) do
    grad = if data > 0, do: value_grad, else: 0
    
    [{child.id, grad}]
  end

  # catch values without children, i.e. intital inputs
  def calculate(%Value{op: nil}, _), do: []
end
```

<!-- livebook:{"output":true} -->

```
{:module, Grad, <<70, 79, 82, 49, 0, 0, 25, ...>>, {:calculate, 2}}
```

This can now be used to calculate the gradients for the example above:

```elixir
left = Value.new(10.0, "left")
right = Value.new(5.0, "right")
result = Math.mul(left, right, "result")

# Assume the current gradient of result is 1
Grad.calculate(result, 1.0)
```

<!-- livebook:{"output":true} -->

```
[{11, 5.0}, {12, 10.0}]
```

## Backpropagation

This here is one major departure from the original implementation of Micrograd. In python the backward function can be added on the instance of the `Value` class and then called to determine the gradients, then updating the state on the instances to keep track of the gradient.

Given the functional approach there isn't the ability to mutate the data. The `Value` "instances" are just immutable key-value maps underneath which can't be modified. While a new value could be created with the gradient set, it wouldn't work when updating the gradient on a deeply nested child in the large computational graphs that will be built.

Instead, this takes a more common functional approach and keeps a separate data structure where the gradient for any value can be looked up by its `id`.

```elixir
defmodule Backward do
  def compute_gradients(value) do
    {nodes, _} = topographic_sort(value, [], MapSet.new())

    Enum.reduce(
      nodes,
      %{value.id => 1.0},
      &gradients_for_node/2
    )
  end

  defp topographic_sort(nil, acc, visited) do
    {acc, visited}
  end
  
  defp topographic_sort(node, acc, visited) do
    if MapSet.member?(visited, node) do
      {acc, visited}
    else
      {acc, visited} = 
        Enum.reduce(
          node.children,
          {acc, MapSet.put(visited, node)},
          fn child, {a, v} -> topographic_sort(child, a, v) end
        )
      
      {[node | acc], visited}
    end
  end

  defp gradients_for_node(node, grad_map) do
    node_grad = Map.get(grad_map, node.id, 0.0)
    child_grads = Grad.calculate(node, node_grad)

    for {child_id, grad} <- child_grads, reduce: grad_map do
      acc ->
        Map.update(acc, child_id, grad, &(&1 + grad))
    end
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, Backward, <<70, 79, 82, 49, 0, 0, 15, ...>>, {:gradients_for_node, 2}}
```

```elixir
grad_map = Backward.compute_gradients(result)
```

<!-- livebook:{"output":true} -->

```
%{11 => 5.0, 12 => 10.0, 13 => 1.0}
```

With the gradients for each of the values available they can be passed to `Mermaid.render_value/2` as the second arguement so the gradients can be viewed in the diagram as well.

```elixir
Mermaid.render_value(result, grad_map)
```

<!-- livebook:{"output":true} -->

```mermaid
---
config:
  theme: base
---
flowchart LR

 11[left
data: 10.0
grad: 5.0]
 --> 1112mul[mul]
 12[right
data: 5.0
grad: 10.0]
 --> 1112mul[mul]
1112mul[mul] --> 13[result
data: 50.0
grad: 1.0]



```

## A single neuron

Zooming into a neural network to look at a single neuron shows they are conceptually modeled on a biological neuron. There are many inputs coming from elsewhere in the network and the result of this neuron is passed onwards to others to act as an input there.

So when modelling a single neuron (also called a perceptron) it needs to take in multiple values, and return a value as the result.

The learning part of the network then comes from the parameters that are used to calculate the result. Each of the inputs coming into the network have an associated `weight` which is multiplied by. Then the results of all of the inputs that have been multiplied by their respective weights are summed together. This result then has a `bias` added to it before being passed to an `activation` function (e.g. `tanh` or `relu`).

The result of the activation function is the output of this single neuron.

For a simple example that has two inputs, the goal is to build a computational graph that looks like this:

```mermaid
---
config:
  look: handDrawn
  theme: natural
---
flowchart LR
input_one --> mul_1[multiply]
weight_one --> mul_1[multiply]
mul_1 --> x1w1["result"]
x1w1 --> sum

input_two --> mul_2[multiply]
weight_two --> mul_2[multiply]
mul_2 --> x2w2["result"]
x2w2 --> sum

sum --> sum_res[result]

sum_res --> add

bias --> add

add --> result

result --> activation

activation --> output
```

```elixir
x1 = Value.new(2.0, "x1")
x2 = Value.new(0.0, "x2")

w1 = Value.new(-3.0, "w1")
w2 = Value.new(1.0, "w2")

# This specific value is chosen to give nice gradients further on
b = Value.new(6.8813735870195432, "b")

x1w1 = Math.mul(x1, w1, "x1*w1")
x2w2 = Math.mul(x2, w2, "x2*w2")

x1w1x2w2 = Math.add(x1w1, x2w2, "x1*w1+x2*w2")

x1w1x2w2b = Math.add(x1w1x2w2, b, "w+b")

output = Math.tanh(x1w1x2w2b, "output")

Mermaid.render_value(output)
```

<!-- livebook:{"output":true} -->

```mermaid
---
config:
  theme: base
---
flowchart LR

 
 
 
 14[x1
data: 2.0
grad: 0.0]
 --> 1416mul[mul]
 16[w1
data: -3.0
grad: 0.0]
 --> 1416mul[mul]
1416mul[mul] --> 19[x1*w1
data: -6.0
grad: 0.0]

 --> 1920add[add]
 
 15[x2
data: 0.0
grad: 0.0]
 --> 1517mul[mul]
 17[w2
data: 1.0
grad: 0.0]
 --> 1517mul[mul]
1517mul[mul] --> 20[x2*w2
data: 0.0
grad: 0.0]

 --> 1920add[add]
1920add[add] --> 21[x1*w1+x2*w2
data: -6.0
grad: 0.0]

 --> 2118add[add]
 18[b
data: 6.881373587019543
grad: 0.0]
 --> 2118add[add]
2118add[add] --> 22[w+b
data: 0.8813735870195432
grad: 0.0]

 --> 22tanh[tanh]
22tanh[tanh] --> 23[output
data: 0.7071067811865476
grad: 0.0]



```

This shows that passing the values `2` and `0` into the calculation as inputs returns approx. `0.7071` as a result, with all of the intermediate values visible along the way.

Passing the `output` `Value` struct (which contains all of the other `Value` structs created as children) into `Backward` creates a new datastruct which holds the gradients of every `Value` in this graph. Passing that to the renderer shows this updated view.

```elixir
grad_map = Backward.compute_gradients(output)

Mermaid.render_value(output, grad_map)
```

<!-- livebook:{"output":true} -->

```mermaid
---
config:
  theme: base
---
flowchart LR

 
 
 
 14[x1
data: 2.0
grad: -1.4999999999999996]
 --> 1416mul[mul]
 16[w1
data: -3.0
grad: 0.9999999999999998]
 --> 1416mul[mul]
1416mul[mul] --> 19[x1*w1
data: -6.0
grad: 0.4999999999999999]

 --> 1920add[add]
 
 15[x2
data: 0.0
grad: 0.4999999999999999]
 --> 1517mul[mul]
 17[w2
data: 1.0
grad: 0.0]
 --> 1517mul[mul]
1517mul[mul] --> 20[x2*w2
data: 0.0
grad: 0.4999999999999999]

 --> 1920add[add]
1920add[add] --> 21[x1*w1+x2*w2
data: -6.0
grad: 0.4999999999999999]

 --> 2118add[add]
 18[b
data: 6.881373587019543
grad: 0.4999999999999999]
 --> 2118add[add]
2118add[add] --> 22[w+b
data: 0.8813735870195432
grad: 0.4999999999999999]

 --> 22tanh[tanh]
22tanh[tanh] --> 23[output
data: 0.7071067811865476
grad: 1.0]



```

## Loss function

```elixir
defmodule Loss do
  def mean_sq_error(predictions, expected) do
    squared_errors =
      for {actual, prediction} <- Enum.zip(expected, predictions) do
        Value.new(-1.0, "a")
        |> Math.mul(actual, "b" )
        |> Math.add(prediction, "c")
        |> Math.pow(Value.new(2, "exp"), "pow")
      end 

    squared_errors
    |> Math.sum("sum")
    |> Math.mul(Value.new(1 / length(predictions), "div"), "avg")
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, Loss, <<70, 79, 82, 49, 0, 0, 10, ...>>, {:mean_sq_error, 2}}
```

```elixir
loss = Loss.mean_sq_error([Value.new(3, "")], [Value.new(-10, "")])
loss.data
```

<!-- livebook:{"output":true} -->

```
169.0
```

```elixir
loss = Loss.mean_sq_error([Value.new(3, "")], [Value.new(3.1, "")])
loss.data
```

<!-- livebook:{"output":true} -->

```
0.010000000000000018
```

```elixir
loss = Loss.mean_sq_error([output], [Value.new(2)])
gradients = Backward.compute_gradients(loss)

# To make the graph easier to read leave out the intemediary values
# from the loss function. Passing in the loss variable unmodified
# will show the full working.
%Value{loss | children: [output], label: "MSE", op: :mse}
|> Mermaid.render_value(gradients)
```

<!-- livebook:{"output":true} -->

```mermaid
---
config:
  theme: base
---
flowchart LR

 
 
 
 
 14[x1
data: 2.0
grad: 3.878679656440357]
 --> 1416mul[mul]
 16[w1
data: -3.0
grad: -2.5857864376269046]
 --> 1416mul[mul]
1416mul[mul] --> 19[x1*w1
data: -6.0
grad: -1.2928932188134523]

 --> 1920add[add]
 
 15[x2
data: 0.0
grad: -1.2928932188134523]
 --> 1517mul[mul]
 17[w2
data: 1.0
grad: -0.0]
 --> 1517mul[mul]
1517mul[mul] --> 20[x2*w2
data: 0.0
grad: -1.2928932188134523]

 --> 1920add[add]
1920add[add] --> 21[x1*w1+x2*w2
data: -6.0
grad: -1.2928932188134523]

 --> 2118add[add]
 18[b
data: 6.881373587019543
grad: -1.2928932188134523]
 --> 2118add[add]
2118add[add] --> 22[w+b
data: 0.8813735870195432
grad: -1.2928932188134523]

 --> 22tanh[tanh]
22tanh[tanh] --> 23[output
data: 0.7071067811865476
grad: -2.585786437626905]

 --> 23mse[mse]
23mse[mse] --> 52[MSE
data: 1.6715728752538102
grad: 1.0]



```

The gradient of output is set to `1` and then the process of moving the gradient backwards through the graph happens. In the middle the addition operations effectively work as a pass through sending the gradient back along through the computation.

At the beginning the gradient of `w1` is the value of `x1` multiplied by the gradient `0.5` passed back. The same is true for `w2`.

### The neuron struct

Another data structure can be created to define a neuron that takes an arbitray number of inputs. This again defines a struct to hold the weights, which is just a list of `Value`s and a bias, which is a single `Value`.

The weights and bias are commonly called the "parameters" of a neural network and will be the values that are adjusted as the network is trained to produce the output expected.

A `new` function on the module handles setting up a neuron with the number of weights required to handle the inputs. The weights a bias are initialised with random numbers to begin with.

The module contains two other fuctions needed to run and train the network.

#### call/2

`call` takes the neuron struct and a list of `Value` as the input. These might be the inputs to the overall network or the outputs from other neurons in the network. Call then runs the same equation shown above to calculate the result of this single perceptron.

#### train/3

This is at the core of how a neural network can "learn" to approximate any function. Here there is a neuron struct being passed in which contains the parameters (weights and bias), the gradient map with the grads for every value in the calculation, and a step size.

In a single training loop each of the parameters is taken and the gradient is looked up, which is its impact on the rate of change of the overall equation with respect to the loss. The loss is then multiplied by the negative step size and that value is added to the parameter.

An example shows how this process leads to a result.

```elixir
defmodule Neuron do
  defstruct [:weights, :bias]
  
  def new(number_of_inputs, opts \\ []) do
    set_random_seed(opts)

    %__MODULE__{
      weights: Enum.map(1..number_of_inputs, & Value.new(random_value(), "w#{&1}")),
      bias: Value.new(random_value(), "b"),
    }
  end

  def call(%__MODULE__{} = neuron, inputs) do
    inputs
    |> Enum.zip(neuron.weights)
    |> Enum.map(fn {x, w} -> Math.mul(x, w, x.label <> "." <> w.label) end)
    |> Math.sum("x.w")
    |> Math.add(neuron.bias, "x.w+b")
    |> Math.tanh("output")
  end

  def train(%__MODULE__{weights: weights, bias: bias} = neuron, gradients, opts \\ []) do
    step_size = Keyword.get(opts, :step_size, 0.01)
    
    updated_weights = 
      Enum.map(weights, fn w ->
        %Value{w | data: w.data + (Map.get(gradients, w.id) * -step_size)}
      end)
    
    %Neuron{neuron |
      weights: updated_weights,
      bias: %Value{bias | data: bias.data + (Map.get(gradients, bias.id) * -step_size)}
    }
  end

  defp set_random_seed(opts) do
    seed = Keyword.get(opts, :seed, Enum.random(1..10000))
    :rand.seed(:default, seed)
  end 
  
  defp random_value() do
    # uniform/0 returns a value between 0 and 1, doubling
    # and subtracting 1 gives a value between -1 and 1
    :rand.uniform() * 2 - 1
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, Neuron, <<70, 79, 82, 49, 0, 0, 30, ...>>, {:random_value, 0}}
```

```elixir
neuron = Neuron.new(2, seed: 1)
```

<!-- livebook:{"output":true} -->

```
%Neuron{
  weights: [
    %Value{data: 0.9739946443328626, op: nil, label: "w1", id: 53, children: []},
    %Value{data: -0.563924456558978, op: nil, label: "w2", id: 54, children: []}
  ],
  bias: %Value{data: 0.38661215006185845, op: nil, label: "b", id: 55, children: []}
}
```

## Training a NOT gate

A logicaly `NOT` gate just returns the inverse of what it is given, i.e. pass in `true` and it returns `false`, pass in `false` and it returns `true`.

With this perceptron model

```elixir
neuron = Neuron.new(1, seed: 42)
```

<!-- livebook:{"output":true} -->

```
%Neuron{
  weights: [%Value{data: -0.2655397043350758, op: nil, label: "w1", id: 56, children: []}],
  bias: %Value{data: 0.7987285881433279, op: nil, label: "b", id: 57, children: []}
}
```

```elixir
output = Neuron.call(neuron, [Value.new(1)])
output.data
```

<!-- livebook:{"output":true} -->

```
0.4878149145799068
```

```elixir
inputs = [
  [Value.new(1, "x1")],
  [Value.new(0, "x1")],
]

expected_outputs = [
  Value.new(1, "x1"),
  Value.new(-1, "x1")
]

predictions = for input <- inputs, do: Neuron.call(neuron, input)
loss = Loss.mean_sq_error(predictions, expected_outputs)

print_results = fn inputs, expected_outputs, predictions, loss ->
  [inputs, expected_outputs, predictions]
  |> Enum.zip()
  |> Enum.each(fn {is, e, p} ->
    is = Enum.map(is, & &1.data) |> Enum.join(", ")

    IO.write(
      "Inputting [#{is}] should return #{e.data}, predicted: #{p.data}\n"
    )
  end)

  IO.write("Loss: #{loss.data}")
end

print_results.(inputs, expected_outputs, predictions, loss)
```

<!-- livebook:{"output":true} -->

```
Inputting [1] should return 1, predicted: 0.4878149145799068
Inputting [0] should return -1, predicted: 0.6633253806816016
Loss: 1.5144924418731918
```

<!-- livebook:{"output":true} -->

```
:ok
```

```elixir
gradients = Backward.compute_gradients(loss)

neuron = Neuron.train(neuron, gradients, step_size: 1)

predictions = for input <- inputs, do: Neuron.call(neuron, input)
loss = Loss.mean_sq_error(predictions, expected_outputs)

print_results.(inputs, expected_outputs, predictions, loss)
```

<!-- livebook:{"output":true} -->

```
Inputting [1] should return 1, predicted: 0.3647337144516438
Inputting [0] should return -1, predicted: 0.25202229446915153
Loss: 0.9855615397011022
```

<!-- livebook:{"output":true} -->

```
:ok
```

```elixir
neuron =
  Enum.reduce(1..10, neuron, fn _, acc -> 
    gradients =
      inputs
      |> Enum.map(& Neuron.call(acc, &1)) 
      |> Loss.mean_sq_error(expected_outputs)
      |> Backward.compute_gradients()
      
    Neuron.train(acc, gradients, step_size: 2)
  end)
  
predictions = Enum.map(inputs, & Neuron.call(neuron, &1)) 
loss = Loss.mean_sq_error(predictions, expected_outputs)

print_results.(inputs, expected_outputs, predictions, loss)
```

<!-- livebook:{"output":true} -->

```
Inputting [1] should return 1, predicted: 0.950672239488215
Inputting [0] should return -1, predicted: -0.9175676587362873
Loss: 0.004614159421662602
```

<!-- livebook:{"output":true} -->

```
:ok
```

```elixir
neuron
```

<!-- livebook:{"output":true} -->

```
%Neuron{
  weights: [%Value{data: 3.4121386403722465, op: nil, label: "w1", id: 56, children: []}],
  bias: %Value{data: -1.5734174832562366, op: nil, label: "b", id: 57, children: []}
}
```

## Training a single neuron to be an AND gate

```elixir
neuron = Neuron.new(2, seed: 1)
```

<!-- livebook:{"output":true} -->

```
%Neuron{
  weights: [
    %Value{data: 0.9739946443328626, op: nil, label: "w1", id: 340, children: []},
    %Value{data: -0.563924456558978, op: nil, label: "w2", id: 341, children: []}
  ],
  bias: %Value{data: 0.38661215006185845, op: nil, label: "b", id: 342, children: []}
}
```

```elixir
inputs = [
  [Value.new(1, "x1"), Value.new(1, "x2")],
  [Value.new(0, "x1"), Value.new(1, "x2")],
  [Value.new(1, "x1"), Value.new(0, "x2")],
  [Value.new(0, "x1"), Value.new(0, "x2")]
]

expected_outputs = [
  Value.new(1),
  Value.new(-1),
  Value.new(-1),
  Value.new(-1)
]

predictions = for input <- inputs, do: Neuron.call(neuron, input)
loss = Loss.mean_sq_error(predictions, expected_outputs)

print_results.(inputs, expected_outputs, predictions, loss)
```

<!-- livebook:{"output":true} -->

```
Inputting [1, 1] should return 1, predicted: 0.6621779257834288
Inputting [0, 1] should return -1, predicted: -0.1754771689583783
Inputting [1, 0] should return -1, predicted: 0.8765337296267863
Inputting [0, 0] should return -1, predicted: 0.3684359230494372
Loss: 1.5469893416640146
```

<!-- livebook:{"output":true} -->

```
:ok
```

```elixir
gradients = Backward.compute_gradients(loss)

neuron = Neuron.train(neuron, gradients, step_size: 2)

predictions = for input <- inputs, do: Neuron.call(neuron, input)
loss = Loss.mean_sq_error(predictions, expected_outputs)

print_results.(inputs, expected_outputs, predictions, loss)
```

<!-- livebook:{"output":true} -->

```
Inputting [1, 1] should return 1, predicted: -0.9794852561162799
Inputting [0, 1] should return -1, predicted: -0.9951875283277666
Inputting [1, 0] should return -1, predicted: -0.8045425111831644
Inputting [0, 0] should return -1, predicted: -0.9508217110732399
Loss: 0.9897517932754194
```

<!-- livebook:{"output":true} -->

```
:ok
```

```elixir
neuron =
  Enum.reduce(1..100, neuron, fn _, acc -> 
    gradients =
      inputs
      |> Enum.map(& Neuron.call(acc, &1)) 
      |> Loss.mean_sq_error(expected_outputs)
      |> Backward.compute_gradients()
      
    Neuron.train(acc, gradients, step_size: 2)
  end)

predictions = Enum.map(inputs, & Neuron.call(neuron, &1)) 
loss = Loss.mean_sq_error(predictions, expected_outputs)

print_results.(inputs, expected_outputs, predictions, loss)
```

<!-- livebook:{"output":true} -->

```
Inputting [1, 1] should return 1, predicted: 0.9201182155991248
Inputting [0, 1] should return -1, predicted: -0.9327180496173629
Inputting [1, 0] should return -1, predicted: -0.9327174930478396
Inputting [0, 0] should return -1, predicted: -0.999899169540279
Loss: 0.0038587265587271685
```

<!-- livebook:{"output":true} -->

```
:ok
```

```elixir
neuron
```

```elixir
neuron
|> Neuron.call([Value.new(1), Value.new(1)])
|> Mermaid.render_value()
```

The trained neural network ends up multiplying each of the inputs by 3, summing them and subtracting 5. Before being passed to `tanh` this leaves `1` for `1 and 1`, `-2` for `1 and 0` and `-5` for `0 and 0`. `tanh` then squishes these values down to close to `1` and `-1` giving approximately the correct answer.
