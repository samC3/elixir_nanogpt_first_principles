<!-- livebook:{"file_entries":[{"name":"gradient.gif","type":"attachment"},{"name":"training_demo_1.gif","type":"attachment"}],"persist_outputs":true} -->

# Neural networks from first principles, Micrograd Elixir

```elixir
Mix.install([
  {:kino, "~> 0.17.0"},
  {:tucan, "~> 0.5.0"},
  {:kino_vega_lite, "~> 0.1.13"}
])
```

## Introduction

Neural networks are these seeminly magically processes that can be applied to a huge array of problems and achieve a passable result. It raises the question though, how do they actually work? What are the fundamentals that we need to understand so that we can build up our knowledge from first principles to then comprehend how more complicated architectures like LLMs work.

In this livebook we will build a neural network from scratch using Elixir, a functional programming language, following along the [brilliant video by Andrej Karpathy](https://www.youtube.com/watch?v=VMj-3S1tku0) where he creates micrograd from scratch in Python. Full credit goes to him for creating the material.

## What are we building

Neural networks able to approximate functions by adjusting the paramaters in the network based on training data.

As a contrived example, we can approximate the result of this mathematical function by giving a small network the input $x$ and result values to learn from.

$$
sin(4x) * cos(5x)
$$

We can see an illustration where the network is begins randomly guessing values for $x$ but over time improves it's answer and gets closer and closer to the actual result. (Note: this is example is "overfitting" the training data, which is something to be avoided in a real example).

<img src="files/training_demo_1.gif" alt="A small neural network being trained to approximate the sin(4x) * cos(5x) function" width="500" />

There are all kinds of libraries (e.g. TensorFlow, PyTorch, Nx, Axon) that give us the tools to build and scale these networks. But if we want to understand them from first principles so that we can have an intuitive sense of what is going on, we don't need to reach for tensors and linear algebra right away. We can build a functional neural network using just straight foward Elixir.

### A functional approach

This Livebook departs in the implementation to demostrate how a neural network can be expressed with a functional language, and how it can result in code that is easier to reason about since the pure functions are explicit at every step along the way.

## Step 1: A computational graph

The great news is the mathematical concepts and formulas we need to understand can be built up from high school level math. Primarily we need basic arthimatic, and certain parts of calculus.

We can think of a neural network as a computational graph which takes a list of numbers and handles the various mathematical operations to produce a list of numbers as an output.

For example, say we wanted to show $2 \times 3 = 6$ as a simple computational graph it would look like this, where it takes in a list containing `2` and `3` as inputs, and produces the output of `6`.

```mermaid
---
config:
  look: handDrawn
  theme: forest
---
flowchart LR
2 --> multiply
3 --> multiply
multiply --> 6
```

In Elixir we can complete mathematical operations in the language as with simple code

```elixir
2 * 3
```

<!-- livebook:{"output":true} -->

```
6
```

Instead of working with raw numbers (integers and floats) we will instead wrap those in a `%Value{}` [struct](https://hexdocs.pm/elixir/structs.html). This will allow us to store the intermediary values to keep track of the calculation we are running.

The `Value` will hold information about a number in the computational graph:

* `data`: the underlying value
* `op`: which mathematical operation created the `Value`. May be `nil` for inputs
* `children`: the `Value` instances used to create this value
* `label`: used to store the name of the variable to be rendered in the Mermaid diagram
* `id`: used when creating the Mermaid diagram

We define a struct with the `defstruct` construct which lists the fields. All we really _need_ to define is:

<!-- livebook:{"force_markdown":true} -->

```elixir
defmodule Value do
  defstruct [:data, :op, :children, :label, :id]
end
```

However we don't want to be concerned about creating the value for `id` for the diagrams as we move further into the livebook so we'll include a `new/2` function as well.

If you've been following along with Andrej Karpathy's course you may have noticed the `grad` field is missing from `Value`. This has been intentionally left out since we'll be taking a slightly different implementation with functional programming, which will be explained in more detail further along.

```elixir
defmodule Value do
  defstruct [:data, :op, :label, :id, children: []]

  def new(data, label \\ "", children \\ [], op \\ nil) do
    %__MODULE__{
      data: data,
      children: children,
      op: op,
      label: label,
      id: :erlang.unique_integer([:monotonic, :positive])
    }
  end
end

Value.new(2.0, "x")
```

<!-- livebook:{"output":true} -->

```
%Value{data: 2.0, op: nil, label: "x", id: 1, children: []}
```

Given these values we can start to make a computational graph like the one we've drawn:

```elixir
two = Value.new(2.0, "two")
three = Value.new(3.0, "three")

Value.new(two.data * three.data, "six", [two, three])
```

<!-- livebook:{"output":true} -->

```
%Value{
  data: 6.0,
  op: nil,
  label: "six",
  id: 4,
  children: [
    %Value{data: 2.0, op: nil, label: "two", id: 2, children: []},
    %Value{data: 3.0, op: nil, label: "three", id: 3, children: []}
  ]
}
```

We also need a way to perform operations on the values so can build up mathimatical functions with our new date model.

We'll create a `Math` module to implement the operations that we will need. You'll notice the `data` field is created using the usual Elixir/Erlang functions to create the values and the rest of the function is building up the metadata for us to use later.

```elixir
defmodule Math do
  def add(value_one, value_two, label) do
    Value.new(
      value_one.data + value_two.data,
      label,
      [value_one, value_two],
      :add
    )
  end

  def mul(value_one, value_two, label) do
    Value.new(
      value_one.data * value_two.data,
      label,
      [value_one, value_two],
      :mul
    )
  end

  def sum(values, label) do
    Value.new(
      Enum.reduce(values, 0.0, & &1.data + &2),
      label,
      values,
      :sum
    )
  end

  def pow(value, exp, label) do
    Value.new(
      :math.pow(value.data, exp.data),
      label,
      [value, exp],
      :pow
    )
  end

  def tanh(value, label) do
    Value.new(
      :math.tanh(value.data),
      label,
      [value],
      :tanh
    )
  end

  def relu(value, label) do
    Value.new(
      Enum.max([value.data, 0]),
      label,
      [value],
      :relu
    )
  end
end

Math.mul(two, three, "six")
```

<!-- livebook:{"output":true} -->

```
%Value{
  data: 6.0,
  op: :mul,
  label: "six",
  id: 5,
  children: [
    %Value{data: 2.0, op: nil, label: "two", id: 2, children: []},
    %Value{data: 3.0, op: nil, label: "three", id: 3, children: []}
  ]
}
```

With the `Math` module we can now create mathematical expressions which we can use to view the computational graphs. What we want to produce programmtically is this with the values of each filled in, and further along the gradients as well.

```mermaid
---
config:
  look: handDrawn
  theme: natural
---
flowchart LR
x --> multiply
y --> multiply
multiply --> add
z --> add
add --> result
```

```elixir
x = Value.new(2.0, "x")
y = Value.new(3.0, "y")
z = Value.new(4.0, "z")

result = Math.mul(x, y, "x*y") |> Math.add(z, "result")
```

<!-- livebook:{"output":true} -->

```
%Value{
  data: 10.0,
  op: :add,
  label: "result",
  id: 10,
  children: [
    %Value{
      data: 6.0,
      op: :mul,
      label: "x*y",
      id: 9,
      children: [
        %Value{data: 2.0, op: nil, label: "x", id: 6, children: []},
        %Value{data: 3.0, op: nil, label: "y", id: 7, children: []}
      ]
    },
    %Value{data: 4.0, op: nil, label: "z", id: 8, children: []}
  ]
}
```

Here we can see the final result of the $x \times y + z$ function is `10`, and we can see all of the intermediate values along the way right back to the intital values.

But this is already not that nice to read and these expressions will only become more complicated, so we will also need a `Mermaid` module to visualize the computational graph in an easier way.

Note: it's not essential to understand this module so feel free to skip over. Also worth noting that this approach will stop working once our computational graphs become sufficiently complicated, but at that stage a full graphic may begin to loses its value.

```elixir
defmodule Mermaid do
  def render_value(value, grad_map \\ %{}) do
    """
    ---
    config:
      theme: base
    ---
    flowchart LR
    #{to_mermaid(value, grad_map)}
    """
    |> Kino.Mermaid.new()
  end

  defp to_mermaid(nil, _grad_map), do: ""

  defp to_mermaid(%{children: []} = value, grad_map) do
    value_node(value, grad_map)
  end

  defp to_mermaid(%{children: children, op: op} = value, grad_map) do
    children_ids = Enum.reduce(children, "", & &2 <> to_string(&1.id))
    operator_node_id = "#{children_ids}#{op}"

    """
    #{
      for child <- children, reduce: "" do
        acc ->
         acc <> "\n #{to_mermaid(child, grad_map)} --> #{operator_node_id}[#{op}]"
      end
    }
    #{operator_node_id}[#{op}] --> #{value_node(value, grad_map)}
    """
  end

  defp value_node(%{id: id, data: data, label: label}, grad_map) do
    """
    #{id}[#{label}
    data: #{data}
    grad: #{Map.get(grad_map, id, 0.0)}]
    """
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, Mermaid, <<70, 79, 82, 49, 0, 0, 18, ...>>, {:value_node, 2}}
```

With this we can now view our result `Value` from before and see the full computational graph. We will fill in the `grad` field in the graph in the next section.

```elixir
Mermaid.render_value(result)
```

<!-- livebook:{"output":true} -->

```mermaid
---
config:
  theme: base
---
flowchart LR

 
 6[x
data: 2.0
grad: 0.0]
 --> 67mul[mul]
 7[y
data: 3.0
grad: 0.0]
 --> 67mul[mul]
67mul[mul] --> 9[x*y
data: 6.0
grad: 0.0]

 --> 98add[add]
 8[z
data: 4.0
grad: 0.0]
 --> 98add[add]
98add[add] --> 10[result
data: 10.0
grad: 0.0]



```

We can use the computational graph to move forwards through the calculation, but we also want to be able to move backwards and determine the gradient for each value.

## Calculus

In order to train neural networks we need to understand the impact of various parts of the calculation so that we can then adjust them accordingly.

The two best sources I found on building an understanding of calculus are:

1. **The essence of calculus** series on youtube by 3Blue1Brown, and
2. **Calculus Made Easy** by Silvanus P. Thompson, first published in 1910 (free pdf copies can be found online)

This quote from *Calculus made easy* explains the first part of what we are looking to get from calculus:

> $d$ which merely means “a little bit of.”
> Thus $dx$ means a little bit of $x$; or $du$ means a little bit of $u$. Ordinary mathematicians think it more polite to say “an element of,”
> instead of “a little bit of.” Just as you please. But you will find that
> these little bits (or elements) may be considered to be indefinitely small.

So we want to look at our calculations and observe what happens when we nudge them by just a little bit. The second part we want to consider is highlighted in this quote:

> Moreover, we are usually dealing with more than one variable at once, and thinking of the way in which one variable depends on the other: ... Or we are asked to consider a rectangle of given area, and to enquire how any increase in the length of it will compel a corresponding decrease in the breadth of it. Or we think of the way in which any variation in the slope of a ladder will cause the height that it reaches, to vary.
> 
> Suppose we have got two such variables that depend one on the other. An alteration in one will bring about an alteration in the other, because of this dependence.

So with this we can think:

1. We have a computational graph with multiple variable that are dependent on each other, if $x$ increases in value,

Then, with the small parts we want to measure that change the occurs to the overall equation. (Here I would highly recommend a detour to the essence of calculus.)

When building neural networks we don't go through the full process of deriving the derivative since it would be an expression with tens of thousands of terms. Instead, we just want an understanding of that the derivative is, and why it's useful in the context of building these networks.

### A single variable function

Lets start with a simple parabola $y = x^2$. What happens to $y$ if we adjusted $x$ by just a tiny amount?

First we can define the function `fn x -> x**2 end` that we can use to calculate the value for `y`

```elixir
f = fn x -> x**2 end

_y = f.(3)
```

<!-- livebook:{"output":true} -->

```
9
```

And with this function calculate the values for $y$ when $x$ is between -10 and 10:

```elixir
xs =
  Enum.map(
    -10..10,
    & %{x: &1, y: f.(&1), type: "curve"}
  )

Tucan.lineplot(xs, "x", "y")
```

<!-- livebook:{"output":true} -->

```vega-lite
{"$schema":"https://vega.github.io/schema/vega-lite/v5.json","__tucan__":{"types":{"type":"nominal","x":"quantitative","y":"quantitative"}},"data":{"values":[{"type":"curve","x":-10,"y":100},{"type":"curve","x":-9,"y":81},{"type":"curve","x":-8,"y":64},{"type":"curve","x":-7,"y":49},{"type":"curve","x":-6,"y":36},{"type":"curve","x":-5,"y":25},{"type":"curve","x":-4,"y":16},{"type":"curve","x":-3,"y":9},{"type":"curve","x":-2,"y":4},{"type":"curve","x":-1,"y":1},{"type":"curve","x":0,"y":0},{"type":"curve","x":1,"y":1},{"type":"curve","x":2,"y":4},{"type":"curve","x":3,"y":9},{"type":"curve","x":4,"y":16},{"type":"curve","x":5,"y":25},{"type":"curve","x":6,"y":36},{"type":"curve","x":7,"y":49},{"type":"curve","x":8,"y":64},{"type":"curve","x":9,"y":81},{"type":"curve","x":10,"y":100}]},"encoding":{"x":{"field":"x","type":"quantitative"},"y":{"field":"y","type":"quantitative"}},"mark":{"fillOpacity":1,"type":"line"}}
```

We can then look to find the derivative by looking at the gradient of the slope an any point along this graph. This is commonly referred to as the _"instantanous rate of change"_, but as Grant points out in the Essence of Calculus, this is a paradox. Can something change is we look at it as a snapshot in single moment of time?

It's more helpful to think of this is how much does it change by between two very very close points in time. Or in this example, how much it change if we made $x$ just a tiny little bit bigger, say by `0.000001`.

That is is what this defintion form wikipedia is saying, as $h$ gets closer and closer to zero what is the impact on the output.

$$
\lim_{h \to 0} \frac{f(x + h) - f(x)}{h}
$$

Which we can write out as:

```elixir
h = 0.000001
x = 3

(f.(x + h) - f.(x)) / h
```

<!-- livebook:{"output":true} -->

```
6.000001000927568
```

Here we can see that when $x = 3$ the _"rate of change"_, or _"gradient"_ of the function when the value is nudged just a little bit it approximately $6$.

If we put that formula into a function and then look at different values of $x$ we can see how the gradient changes for large and smaller values.

```elixir
calc_grad = fn x, h -> (f.(x + h) - f.(x)) / h end

tangent_line =
  fn x, h ->
    Enum.map(-2..2, fn i ->
      grad = calc_grad.(x, h)
      %{x: x + i, y: f.(x) + grad * ((x + i) - x), type: "tangent"}
    end)
  end

Enum.map([2, 0.4, -3], fn x ->
  Tucan.lineplot(
    xs ++ tangent_line.(x, h),
    "x",
    "y",
    color_by: "type",
    title: "grad is #{Float.round(calc_grad.(x, h), 1)} when x is #{x}"
  )
end)
|> Tucan.hconcat()
```

<!-- livebook:{"output":true} -->

```vega-lite
{"$schema":"https://vega.github.io/schema/vega-lite/v5.json","hconcat":[{"__tucan__":{"types":{"type":"nominal","x":"quantitative","y":"quantitative"}},"data":{"values":[{"type":"curve","x":-10,"y":100},{"type":"curve","x":-9,"y":81},{"type":"curve","x":-8,"y":64},{"type":"curve","x":-7,"y":49},{"type":"curve","x":-6,"y":36},{"type":"curve","x":-5,"y":25},{"type":"curve","x":-4,"y":16},{"type":"curve","x":-3,"y":9},{"type":"curve","x":-2,"y":4},{"type":"curve","x":-1,"y":1},{"type":"curve","x":0,"y":0},{"type":"curve","x":1,"y":1},{"type":"curve","x":2,"y":4},{"type":"curve","x":3,"y":9},{"type":"curve","x":4,"y":16},{"type":"curve","x":5,"y":25},{"type":"curve","x":6,"y":36},{"type":"curve","x":7,"y":49},{"type":"curve","x":8,"y":64},{"type":"curve","x":9,"y":81},{"type":"curve","x":10,"y":100},{"type":"tangent","x":0,"y":-4.000002001296025},{"type":"tangent","x":1,"y":-1.000648012450256e-6},{"type":"tangent","x":2,"y":4.0},{"type":"tangent","x":3,"y":8.000001000648012},{"type":"tangent","x":4,"y":12.000002001296025}]},"encoding":{"color":{"field":"type"},"x":{"field":"x","type":"quantitative"},"y":{"field":"y","type":"quantitative"}},"mark":{"fillOpacity":1,"type":"line"},"title":"grad is 4.0 when x is 2"},{"__tucan__":{"types":{"type":"nominal","x":"quantitative","y":"quantitative"}},"data":{"values":[{"type":"curve","x":-10,"y":100},{"type":"curve","x":-9,"y":81},{"type":"curve","x":-8,"y":64},{"type":"curve","x":-7,"y":49},{"type":"curve","x":-6,"y":36},{"type":"curve","x":-5,"y":25},{"type":"curve","x":-4,"y":16},{"type":"curve","x":-3,"y":9},{"type":"curve","x":-2,"y":4},{"type":"curve","x":-1,"y":1},{"type":"curve","x":0,"y":0},{"type":"curve","x":1,"y":1},{"type":"curve","x":2,"y":4},{"type":"curve","x":3,"y":9},{"type":"curve","x":4,"y":16},{"type":"curve","x":5,"y":25},{"type":"curve","x":6,"y":36},{"type":"curve","x":7,"y":49},{"type":"curve","x":8,"y":64},{"type":"curve","x":9,"y":81},{"type":"curve","x":10,"y":100},{"type":"tangent","x":-1.6,"y":-1.4400019999462543},{"type":"tangent","x":-0.6,"y":-0.6400009999731272},{"type":"tangent","x":0.4,"y":0.16000000000000003},{"type":"tangent","x":1.4,"y":0.9600009999731272},{"type":"tangent","x":2.4,"y":1.7600019999462546}]},"encoding":{"color":{"field":"type"},"x":{"field":"x","type":"quantitative"},"y":{"field":"y","type":"quantitative"}},"mark":{"fillOpacity":1,"type":"line"},"title":"grad is 0.8 when x is 0.4"},{"__tucan__":{"types":{"type":"nominal","x":"quantitative","y":"quantitative"}},"data":{"values":[{"type":"curve","x":-10,"y":100},{"type":"curve","x":-9,"y":81},{"type":"curve","x":-8,"y":64},{"type":"curve","x":-7,"y":49},{"type":"curve","x":-6,"y":36},{"type":"curve","x":-5,"y":25},{"type":"curve","x":-4,"y":16},{"type":"curve","x":-3,"y":9},{"type":"curve","x":-2,"y":4},{"type":"curve","x":-1,"y":1},{"type":"curve","x":0,"y":0},{"type":"curve","x":1,"y":1},{"type":"curve","x":2,"y":4},{"type":"curve","x":3,"y":9},{"type":"curve","x":4,"y":16},{"type":"curve","x":5,"y":25},{"type":"curve","x":6,"y":36},{"type":"curve","x":7,"y":49},{"type":"curve","x":8,"y":64},{"type":"curve","x":9,"y":81},{"type":"curve","x":10,"y":100},{"type":"tangent","x":-5,"y":20.999998001499534},{"type":"tangent","x":-4,"y":14.999999000749767},{"type":"tangent","x":-3,"y":9.0},{"type":"tangent","x":-2,"y":3.000000999250233},{"type":"tangent","x":-1,"y":-2.9999980014995344}]},"encoding":{"color":{"field":"type"},"x":{"field":"x","type":"quantitative"},"y":{"field":"y","type":"quantitative"}},"mark":{"fillOpacity":1,"type":"line"},"title":"grad is -6.0 when x is -3"}]}
```

Here we can see the gradient rendered as a slope on the parabola to visualize the "steepness", i.e. the higher rate of change as $x$ grows larger. Notice that we get a negative slope when $x$ is a negative value.

<!-- livebook:{"break_markdown":true} -->

### Multi variable functions

Say we have a calcualtion that takes three variables instead of one, e.g. $x \times y + z$ we now want to know what happens when $x$, $y$ or $z$ are adjusted just a small amount and how that impacts the result.

Lets create a function so we can calculate the result:

```elixir
x = 8
y = 10
z = 4

f = fn x, y, z -> x * y + z end

f.(x, y, z)
```

<!-- livebook:{"output":true} -->

```
84
```

Now we can increase each variable separately using the same approach above to see the resulting gradient.

```elixir
h = 0.000000001

x_grad = (f.(x + h, y, z) - f.(x, y, z)) / h
y_grad = (f.(x, y + h, z) - f.(x, y, z)) / h
z_grad = (f.(x, y, z + h) - f.(x, y, z)) / h

IO.write """
gradient of x: #{x_grad}
gradient of y: #{y_grad}
gradient of z: #{z_grad}
"""
```

<!-- livebook:{"output":true} -->

```
gradient of x: 10.000007932831068
gradient of y: 8.000000661922968
gradient of z: 1.0000036354540498
```

<!-- livebook:{"output":true} -->

```
:ok
```

We can make some interesting observations about the results of these gradients:

1. The gradient of $z$ is always close to 1 no matter the value
2. The gradient of $x$ is approximately the value $y$, and vice-versa

#### Addition rule

We can prove this result algebraically. If we have this simple equation where we want to find the derivative of $c$ with respect to $b$, usually written as $\frac{dc}{db}$

$$
c = a + b
$$

A small amount called $db$ is added to $b$ and the same is done for $c$ (called $dc$).

$$
c + dc = a + b + db 
$$

Our goal is to find the value $\frac{dc}{db}$ so we need to remove the unnesseary values. Since we know $c$ is equal to $a + b$ we can subtract $c$ from the left hand side and $a + b$ from the right hand side to simplify the calculation, which leaves us with:

$$
dc = db
$$

We then divide both sides by $db$ to arrive at our answer for $\frac{dc}{db}$, which will be one given $db$ divided by itself is equal to one

$$
\frac{dc}{db} = 1
$$

#### Product rule

If we have the same equation but multiplied instead we can prove how the gradient of one variable will be the value of the other. To solve for $\frac{dc}{db}$ for this equation:

$$
c = a \times b
$$

We add the small amounts for $b$ and $c$

$$
c + dc = a \times b + db
$$

Again, we subtract the original $c = a \times b$ from each side to leave us with:

$$
dc = a \times db
$$

Dividing each side by $db$ we get:

$$
\frac{dc}{db} = a
$$

If we repeated the process by solving for $\frac{dc}{da}$ we would find that returns $b$ as the result, proving the product rule.

#### Power rule

We can prove the derivative for an exponent following the same process algebraically. Starting with:

$$
b = a^2
$$

We add a small amount to $a$ and the correseponding change to $b$

$$
b + db = (a + da)^2
$$

Expanding this out we can look at the sqaure of the $a + da$ as it multiplying by itself.

$$
b + db = (a + da) \times (a + da)
$$

Which we can expand with the binomial therom:

$$
b + db = a^2 + (2 \times a \times da) + da^2
$$

At this point we can ignore $da^2$ since a small, almost zero number multiplied by itself becomes an even smaller number. So its impact on the overall equation is negligable.

$$
b + db = a^2 + (2 \times a \times da)
$$

We then subtract the original equation

$$
db = 2 \times a \times da
$$

And finally divide both sides by $da$ to get the result of $\frac{dc}{da}$

$$
\frac{db}{da} = 2a
$$

If we did the same process for $b = a^3$ we would find the result to $\frac{dc}{da} = 3a^2$.

The pattern here is the derivative of $a^n$ will be $na^{n-1}$

<!-- livebook:{"break_markdown":true} -->

#### Chain rule

_Calculus Made Easy_ describes the chain rule as "a useful dodge" which is perfect for what we need. It allows us to easily find the rate of change between different values in a complicated function.

The chain rule is simply this:

$$
\frac{dz}{dx} = \frac{dz}{dy} . \frac{dy}{dx}
$$

Wikipedia has a useful explanation:

> In calculus, the chain rule is a formula that expresses the derivative of the composition of two differentiable functions f and g in terms of the derivatives of f and g.
> 
> ...
> 
> Intuitively, the chain rule states that knowing the instantaneous rate of change of z relative to y and that of y relative to x allows one to calculate the instantaneous rate of change of z relative to x as the product of the two rates of change.
> 
> If a car travels twice as fast as a bicycle and the bicycle is four times as fast as a walking man, then the car travels 2 × 4 = 8 times as fast as the man.

[Wikipedia Chain Rule](https://en.wikipedia.org/wiki/Chain_rule)

Thinking of it in programmatically, we know that our equation $x \times y + z$ could also be written as: `Math.add(Math.mul(x, y), z)`.

<!-- livebook:{"break_markdown":true} -->

With this knowledge we can now create a `Grad` module that takes in a `Value` and determines the gradients of it's children, i.e. the values that created the one passed in.

So for example if we had this simplifed value:

<!-- livebook:{"force_markdown":true} -->

```elixir
%Value{
  id: 3,
  op: :mul,
  data: 50,
  children:[
    %Value{data: 5, id: 1},
    %Value{data: 10, id: 2}
  ]
}
```

The `Grad` module can automatically determine the gradients of each child and return them with their IDs:

<!-- livebook:{"force_markdown":true} -->

```elixir
[{1, 10}, {2, 5}]
```

Note: the use of the term children here can be confusing since really, these are the parent values that created the Value being passed in. I believe Andrej will have used the term children since the final value of the calculation is effectively the root node in the graph, and the previous values branch away as it's child nodes. I've left the name the same here to make it easier to compare back to the original micrograd library.

```elixir
defmodule Grad do
  # As shown above the gradient from addition results in 1. Then given the
  # chain rule we multiply the gradients coming from the parent to
  # become value_grad * 1. So we're just passing through the gradient
  # from the upstream value to the children.
  def calculate(%Value{op: :add, children: [left, right]}, value_grad) do
    [
      {left.id, value_grad},
      {right.id, value_grad}
    ]
  end

  # The same as above but passing to all of the children
  def calculate(%Value{op: :sum, children: children}, value_grad) do
    Enum.map(children, & {&1.id, value_grad})
  end

  # As the product rule above proved the gradient of each child is the 
  # value of the other child. Then for the chain rule we multiply by
  # the value gradient to get the final gradient value for each child.
  def calculate(%Value{op: :mul, children: [left, right]}, value_grad) do
    [
      {left.id, right.data * value_grad},
      {right.id, left.data * value_grad}
    ]
  end

  # This is power rule described above then multiplied by the value_grad
  def calculate(%Value{op: :pow, children: [child, exp]}, value_grad) do
    [
      {child.id, (exp.data * child.data ** (exp.data - 1)) * value_grad}
    ]
  end

  # For the sake of brevity I haven't included the quotient rule or the working
  # for tanh, but wikipedia has a good write to explain the process.
  def calculate(%Value{op: :tanh, children: [child], data: data}, value_grad) do
    [
      {child.id, (1 - data ** 2) * value_grad}
    ]
  end

  def calculate(%Value{op: :relu, children: [child], data: data}, value_grad) do
    grad = if data > 0, do: value_grad, else: 0
    
    [{child.id, grad}]
  end

  # catch values without children, i.e. intital inputs
  def calculate(%Value{op: nil}, _), do: []
end
```

<!-- livebook:{"output":true} -->

```
{:module, Grad, <<70, 79, 82, 49, 0, 0, 25, ...>>, {:calculate, 2}}
```

We can now use this to calculate the gradients for our example above:

```elixir
left = Value.new(10.0, "left")
right = Value.new(5.0, "right")
result = Math.mul(left, right, "result")

# Assume the current gradient of result is 1
Grad.calculate(result, 1.0)
```

<!-- livebook:{"output":true} -->

```
[{11, 5.0}, {12, 10.0}]
```

## Backpropagation

This here is one major departure from the original implementation of Micrograd. In python we can add a backward function on the instance of the `Value` class and then call it to determine the gradients, then updating the state on the instances to keep track of the gradient.

Given our functional approach we don't have the ability to mutate the data. Our `Value` "instances" are just immutable key-value maps underneath which we can't modify. While we could create a new value with the gradient set, it wouldn't work when we're updating the gradient on a deeply nested child in the large computational graphs we are going to be building.

Instead, we'll take a more common functional approach and keep a separate data structure where we can lookup the gradient for any value by its `id`.

```elixir
defmodule Backward do
  def compute_gradients(value) do
    {nodes, _} = topographic_sort(value, [], MapSet.new())

    Enum.reduce(
      nodes,
      %{value.id => 1.0},
      &gradients_for_node/2
    )
  end

  defp topographic_sort(nil, acc, visited) do
    {acc, visited}
  end
  
  defp topographic_sort(node, acc, visited) do
    if MapSet.member?(visited, node) do
      {acc, visited}
    else
      {acc, visited} = 
        Enum.reduce(
          node.children,
          {acc, MapSet.put(visited, node)},
          fn child, {a, v} -> topographic_sort(child, a, v) end
        )
      
      {[node | acc], visited}
    end
  end

  defp gradients_for_node(node, grad_map) do
    node_grad = Map.get(grad_map, node.id, 0.0)
    child_grads = Grad.calculate(node, node_grad)

    for {child_id, grad} <- child_grads, reduce: grad_map do
      acc ->
        Map.update(acc, child_id, grad, &(&1 + grad))
    end
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, Backward, <<70, 79, 82, 49, 0, 0, 15, ...>>, {:gradients_for_node, 2}}
```

```elixir
grad_map = Backward.compute_gradients(result)
```

<!-- livebook:{"output":true} -->

```
%{11 => 5.0, 12 => 10.0, 13 => 1.0}
```

Now that we have the gradients for each of the values we can pass that to `Mermaid.render_value/2` as the second arguement so we can view the gradients in our diagram as well.

```elixir
Mermaid.render_value(result, grad_map)
```

<!-- livebook:{"output":true} -->

```mermaid
---
config:
  theme: base
---
flowchart LR

 11[left
data: 10.0
grad: 5.0]
 --> 1112mul[mul]
 12[right
data: 5.0
grad: 10.0]
 --> 1112mul[mul]
1112mul[mul] --> 13[result
data: 50.0
grad: 1.0]



```

## A single neuron

If we zoom into a neural network to look at a single neuron we can see they are at conceptually model on a biological neuron. There are many inputs coming from elsewhere in the network and the result of this neuron is passed onwards to others to act as an input there.

So when modelling a single neuron (also called a perceptron) we need to take in multiple values, and return a value as the result.

The learning part of our network then comes from the parameters that are used to calculate the result. Each of the inputs coming into the network have an associated `weight` which is multiplied by. Then we take the results of all of the inputs that have been multiplied by their respective weights and sum them together. This result then has a `bias` added to it before being passed to an `activation` function (e.g. `tanh` or `relu`).

The result of the activation function is the output of this single neuron.

For a simple example that has two inputs, we want to build a computational graph that looks like this:

```mermaid
---
config:
  look: handDrawn
  theme: natural
---
flowchart LR
input_one --> mul_1[multiply]
weight_one --> mul_1[multiply]
mul_1 --> x1w1["result"]
x1w1 --> sum

input_two --> mul_2[multiply]
weight_two --> mul_2[multiply]
mul_2 --> x2w2["result"]
x2w2 --> sum

sum --> sum_res[result]

sum_res --> add

bias --> add

add --> result

result --> activation

activation --> output
```

```elixir
x1 = Value.new(2.0, "x1")
x2 = Value.new(0.0, "x2")

w1 = Value.new(-3.0, "w1")
w2 = Value.new(1.0, "w2")

# This specific value is so we get nice gradients further on
b = Value.new(6.8813735870195432, "b")

x1w1 = Math.mul(x1, w1, "x1*w1")
x2w2 = Math.mul(x2, w2, "x2*w2")

x1w1x2w2 = Math.add(x1w1, x2w2, "x1*w1+x2*w2")

x1w1x2w2b = Math.add(x1w1x2w2, b, "w+b")

output = Math.tanh(x1w1x2w2b, "output")

Mermaid.render_value(output)
```

<!-- livebook:{"output":true} -->

```mermaid
---
config:
  theme: base
---
flowchart LR

 
 
 
 14[x1
data: 2.0
grad: 0.0]
 --> 1416mul[mul]
 16[w1
data: -3.0
grad: 0.0]
 --> 1416mul[mul]
1416mul[mul] --> 19[x1*w1
data: -6.0
grad: 0.0]

 --> 1920add[add]
 
 15[x2
data: 0.0
grad: 0.0]
 --> 1517mul[mul]
 17[w2
data: 1.0
grad: 0.0]
 --> 1517mul[mul]
1517mul[mul] --> 20[x2*w2
data: 0.0
grad: 0.0]

 --> 1920add[add]
1920add[add] --> 21[x1*w1+x2*w2
data: -6.0
grad: 0.0]

 --> 2118add[add]
 18[b
data: 6.881373587019543
grad: 0.0]
 --> 2118add[add]
2118add[add] --> 22[w+b
data: 0.8813735870195432
grad: 0.0]

 --> 22tanh[tanh]
22tanh[tanh] --> 23[output
data: 0.7071067811865476
grad: 0.0]



```

Here we can see that if we pass the values `2` and `0` into the calculation as inputs we get approx. `0.7071` as a result. We can also see all of the intermediate values along the way.

If we then pass the `output` `Value` struct (which contains all of the other `Value` structs created as children) into `Backward` we can create a new datastruct which holds the gradients of every `Value` in this graph. Passing that to our renderer shows us this update view.

```elixir
grad_map = Backward.compute_gradients(output)

Mermaid.render_value(output, grad_map)
```

<!-- livebook:{"output":true} -->

```mermaid
---
config:
  theme: base
---
flowchart LR

 
 
 
 14[x1
data: 2.0
grad: -1.4999999999999996]
 --> 1416mul[mul]
 16[w1
data: -3.0
grad: 0.9999999999999998]
 --> 1416mul[mul]
1416mul[mul] --> 19[x1*w1
data: -6.0
grad: 0.4999999999999999]

 --> 1920add[add]
 
 15[x2
data: 0.0
grad: 0.4999999999999999]
 --> 1517mul[mul]
 17[w2
data: 1.0
grad: 0.0]
 --> 1517mul[mul]
1517mul[mul] --> 20[x2*w2
data: 0.0
grad: 0.4999999999999999]

 --> 1920add[add]
1920add[add] --> 21[x1*w1+x2*w2
data: -6.0
grad: 0.4999999999999999]

 --> 2118add[add]
 18[b
data: 6.881373587019543
grad: 0.4999999999999999]
 --> 2118add[add]
2118add[add] --> 22[w+b
data: 0.8813735870195432
grad: 0.4999999999999999]

 --> 22tanh[tanh]
22tanh[tanh] --> 23[output
data: 0.7071067811865476
grad: 1.0]



```

## Loss function

```elixir
defmodule Loss do
  def mean_sq_error(predictions, expected) do
    squared_errors =
      for {actual, prediction} <- Enum.zip(expected, predictions) do
        Value.new(-1.0, "a")
        |> Math.mul(actual, "b" )
        |> Math.add(prediction, "c")
        |> Math.pow(Value.new(2, "exp"), "pow")
      end 

    squared_errors
    |> Math.sum("sum")
    |> Math.mul(Value.new(1 / length(predictions), "div"), "avg")
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, Loss, <<70, 79, 82, 49, 0, 0, 10, ...>>, {:mean_sq_error, 2}}
```

```elixir
loss = Loss.mean_sq_error([Value.new(3, "")], [Value.new(-10, "")])
loss.data
```

<!-- livebook:{"output":true} -->

```
169.0
```

```elixir
loss = Loss.mean_sq_error([Value.new(3, "")], [Value.new(3.1, "")])
loss.data
```

<!-- livebook:{"output":true} -->

```
0.010000000000000018
```

```elixir
loss = Loss.mean_sq_error([output], [Value.new(2)])
gradients = Backward.compute_gradients(loss)

# To make the graph easier to read leave out the intemediary values
# from the loss function. Passing in the loss variable unmodified
# will show the full working.
%Value{loss | children: [output], label: "MSE", op: :mse}
|> Mermaid.render_value(gradients)
```

<!-- livebook:{"output":true} -->

```mermaid
---
config:
  theme: base
---
flowchart LR

 
 
 
 
 14[x1
data: 2.0
grad: 3.878679656440357]
 --> 1416mul[mul]
 16[w1
data: -3.0
grad: -2.5857864376269046]
 --> 1416mul[mul]
1416mul[mul] --> 19[x1*w1
data: -6.0
grad: -1.2928932188134523]

 --> 1920add[add]
 
 15[x2
data: 0.0
grad: -1.2928932188134523]
 --> 1517mul[mul]
 17[w2
data: 1.0
grad: -0.0]
 --> 1517mul[mul]
1517mul[mul] --> 20[x2*w2
data: 0.0
grad: -1.2928932188134523]

 --> 1920add[add]
1920add[add] --> 21[x1*w1+x2*w2
data: -6.0
grad: -1.2928932188134523]

 --> 2118add[add]
 18[b
data: 6.881373587019543
grad: -1.2928932188134523]
 --> 2118add[add]
2118add[add] --> 22[w+b
data: 0.8813735870195432
grad: -1.2928932188134523]

 --> 22tanh[tanh]
22tanh[tanh] --> 23[output
data: 0.7071067811865476
grad: -2.585786437626905]

 --> 23mse[mse]
23mse[mse] --> 52[MSE
data: 1.6715728752538102
grad: 1.0]



```

The gradient of output is set to `1` and then the process of moving the gradient backwards through the graph happens. In the middle we see that the addition operations effectively work as a pass through sending the gradient back along through the computation.

Then at the beginning the gradient of `w1` is the value of `x1` multiplied by the gradient `0.5` passed back. The same is true for `w2`.

### The neuron struct

We can create another data structure to define a neuron that takes an arbitray number of inputs. We'll again define a struct to hold the weights, which is just a list of `Value`s and a bias, which is a single `Value`.

The weights and bias are commonly called the "parameters" of a neural network and will be the values that are adjusted as we train the network to produce the output we expect.

We'll create a `new` function on the module to handle setting up a neuron with the number of weights required to handle the inputs. The weights a bias are initialised with random numbers to begin with.

The module contains two other fuctions we need to run and train the network.

#### call/2

`call` takes the neuron struct and a list of `Value` as the input. These might be the inputs to the overall network or the outputs from other neurons in the network. Call then runs the same equation we did above to calculate the result of this single perceptron.

#### train/3

This is at the core of how a neural network can "learn" to approximate any function. Here we have neuron struct being passed in which contains the parameters (weights and bias), the gradient map with the grads for every value in the calculation, and a step size.

When we do a single training loop we take each of the parameters and lookup the gradient, which is it's impact on the rate of change of the overall equation with respect to the loss. We then multiply the loss by the negative step size and add that value from the parameter.

We'll work through an example to show how this process leads to a result.

```elixir
defmodule Neuron do
  defstruct [:weights, :bias]
  
  def new(number_of_inputs, opts \\ []) do
    set_random_seed(opts)

    %__MODULE__{
      weights: Enum.map(1..number_of_inputs, & Value.new(random_value(), "w#{&1}")),
      bias: Value.new(random_value(), "b"),
    }
  end

  def call(%__MODULE__{} = neuron, inputs) do
    inputs
    |> Enum.zip(neuron.weights)
    |> Enum.map(fn {x, w} -> Math.mul(x, w, x.label <> "." <> w.label) end)
    |> Math.sum("x.w")
    |> Math.add(neuron.bias, "x.w+b")
    |> Math.tanh("output")
  end

  def train(%__MODULE__{weights: weights, bias: bias} = neuron, gradients, opts \\ []) do
    step_size = Keyword.get(opts, :step_size, 0.01)
    
    updated_weights = 
      Enum.map(weights, fn w ->
        %Value{w | data: w.data + (Map.get(gradients, w.id) * -step_size)}
      end)
    
    %Neuron{neuron |
      weights: updated_weights,
      bias: %Value{bias | data: bias.data + (Map.get(gradients, bias.id) * -step_size)}
    }
  end

  defp set_random_seed(opts) do
    seed = Keyword.get(opts, :seed, Enum.random(1..10000))
    :rand.seed(:default, seed)
  end 
  
  defp random_value() do
    # uniform/0 returns a value between 0 and 1, doubling
    # and subtracting 1 gives us a value between -1 and 1
    :rand.uniform() * 2 - 1
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, Neuron, <<70, 79, 82, 49, 0, 0, 30, ...>>, {:random_value, 0}}
```

```elixir
neuron = Neuron.new(2, seed: 1)
```

<!-- livebook:{"output":true} -->

```
%Neuron{
  weights: [
    %Value{data: 0.9739946443328626, op: nil, label: "w1", id: 53, children: []},
    %Value{data: -0.563924456558978, op: nil, label: "w2", id: 54, children: []}
  ],
  bias: %Value{data: 0.38661215006185845, op: nil, label: "b", id: 55, children: []}
}
```

## Training a NOT gate

A logicaly `NOT` gate just returns the inverse of what it is given, i.e. pass in `true` and it returns `false`, pass in `false` and it returns `true`.

With our perceptron model

```elixir
neuron = Neuron.new(1, seed: 42)
```

<!-- livebook:{"output":true} -->

```
%Neuron{
  weights: [%Value{data: -0.2655397043350758, op: nil, label: "w1", id: 56, children: []}],
  bias: %Value{data: 0.7987285881433279, op: nil, label: "b", id: 57, children: []}
}
```

```elixir
output = Neuron.call(neuron, [Value.new(1)])
output.data
```

<!-- livebook:{"output":true} -->

```
0.4878149145799068
```

```elixir
inputs = [
  [Value.new(1, "x1")],
  [Value.new(0, "x1")],
]

expected_outputs = [
  Value.new(1, "x1"),
  Value.new(-1, "x1")
]

predictions = for input <- inputs, do: Neuron.call(neuron, input)
loss = Loss.mean_sq_error(predictions, expected_outputs)

print_results = fn inputs, expected_outputs, predictions, loss ->
  [inputs, expected_outputs, predictions]
  |> Enum.zip()
  |> Enum.each(fn {is, e, p} ->
    is = Enum.map(is, & &1.data) |> Enum.join(", ")

    IO.write(
      "Inputting [#{is}] should return #{e.data}, predicted: #{p.data}\n"
    )
  end)

  IO.write("Loss: #{loss.data}")
end

print_results.(inputs, expected_outputs, predictions, loss)
```

<!-- livebook:{"output":true} -->

```
Inputting [1] should return 1, predicted: 0.4878149145799068
Inputting [0] should return -1, predicted: 0.6633253806816016
Loss: 1.5144924418731918
```

<!-- livebook:{"output":true} -->

```
:ok
```

```elixir
gradients = Backward.compute_gradients(loss)

neuron = Neuron.train(neuron, gradients, step_size: 1)

predictions = for input <- inputs, do: Neuron.call(neuron, input)
loss = Loss.mean_sq_error(predictions, expected_outputs)

print_results.(inputs, expected_outputs, predictions, loss)
```

<!-- livebook:{"output":true} -->

```
Inputting [1] should return 1, predicted: 0.3647337144516438
Inputting [0] should return -1, predicted: 0.25202229446915153
Loss: 0.9855615397011022
```

<!-- livebook:{"output":true} -->

```
:ok
```

```elixir
neuron =
  Enum.reduce(1..10, neuron, fn _, acc -> 
    gradients =
      inputs
      |> Enum.map(& Neuron.call(acc, &1)) 
      |> Loss.mean_sq_error(expected_outputs)
      |> Backward.compute_gradients()
      
    Neuron.train(acc, gradients, step_size: 2)
  end)
  
predictions = Enum.map(inputs, & Neuron.call(neuron, &1)) 
loss = Loss.mean_sq_error(predictions, expected_outputs)

print_results.(inputs, expected_outputs, predictions, loss)
```

<!-- livebook:{"output":true} -->

```
Inputting [1] should return 1, predicted: 0.950672239488215
Inputting [0] should return -1, predicted: -0.9175676587362873
Loss: 0.004614159421662602
```

<!-- livebook:{"output":true} -->

```
:ok
```

```elixir
neuron
```

<!-- livebook:{"output":true} -->

```
%Neuron{
  weights: [%Value{data: 3.4121386403722465, op: nil, label: "w1", id: 56, children: []}],
  bias: %Value{data: -1.5734174832562366, op: nil, label: "b", id: 57, children: []}
}
```

## Training a single neuron to be an AND gate

```elixir
neuron = Neuron.new(2, seed: 1)
```

<!-- livebook:{"output":true} -->

```
%Neuron{
  weights: [
    %Value{data: 0.9739946443328626, op: nil, label: "w1", id: 340, children: []},
    %Value{data: -0.563924456558978, op: nil, label: "w2", id: 341, children: []}
  ],
  bias: %Value{data: 0.38661215006185845, op: nil, label: "b", id: 342, children: []}
}
```

```elixir
inputs = [
  [Value.new(1, "x1"), Value.new(1, "x2")],
  [Value.new(0, "x1"), Value.new(1, "x2")],
  [Value.new(1, "x1"), Value.new(0, "x2")],
  [Value.new(0, "x1"), Value.new(0, "x2")]
]

expected_outputs = [
  Value.new(1),
  Value.new(-1),
  Value.new(-1),
  Value.new(-1)
]

predictions = for input <- inputs, do: Neuron.call(neuron, input)
loss = Loss.mean_sq_error(predictions, expected_outputs)

print_results.(inputs, expected_outputs, predictions, loss)
```

<!-- livebook:{"output":true} -->

```
Inputting [1, 1] should return 1, predicted: 0.6621779257834288
Inputting [0, 1] should return -1, predicted: -0.1754771689583783
Inputting [1, 0] should return -1, predicted: 0.8765337296267863
Inputting [0, 0] should return -1, predicted: 0.3684359230494372
Loss: 1.5469893416640146
```

<!-- livebook:{"output":true} -->

```
:ok
```

```elixir
gradients = Backward.compute_gradients(loss)

neuron = Neuron.train(neuron, gradients, step_size: 2)

predictions = for input <- inputs, do: Neuron.call(neuron, input)
loss = Loss.mean_sq_error(predictions, expected_outputs)

print_results.(inputs, expected_outputs, predictions, loss)
```

<!-- livebook:{"output":true} -->

```
Inputting [1, 1] should return 1, predicted: -0.9794852561162799
Inputting [0, 1] should return -1, predicted: -0.9951875283277666
Inputting [1, 0] should return -1, predicted: -0.8045425111831644
Inputting [0, 0] should return -1, predicted: -0.9508217110732399
Loss: 0.9897517932754194
```

<!-- livebook:{"output":true} -->

```
:ok
```

```elixir
neuron =
  Enum.reduce(1..100, neuron, fn _, acc -> 
    gradients =
      inputs
      |> Enum.map(& Neuron.call(acc, &1)) 
      |> Loss.mean_sq_error(expected_outputs)
      |> Backward.compute_gradients()
      
    Neuron.train(acc, gradients, step_size: 2)
  end)

predictions = Enum.map(inputs, & Neuron.call(neuron, &1)) 
loss = Loss.mean_sq_error(predictions, expected_outputs)

print_results.(inputs, expected_outputs, predictions, loss)
```

<!-- livebook:{"output":true} -->

```
Inputting [1, 1] should return 1, predicted: 0.9201182155991248
Inputting [0, 1] should return -1, predicted: -0.9327180496173629
Inputting [1, 0] should return -1, predicted: -0.9327174930478396
Inputting [0, 0] should return -1, predicted: -0.999899169540279
Loss: 0.0038587265587271685
```

<!-- livebook:{"output":true} -->

```
:ok
```

```elixir
neuron
```

<!-- livebook:{"output":true} -->

```
%Neuron{
  weights: [
    %Value{data: 3.268692394832694, op: nil, label: "w1", id: 340, children: []},
    %Value{data: 3.2686881147652467, op: nil, label: "w2", id: 341, children: []}
  ],
  bias: %Value{data: -4.947583416130867, op: nil, label: "b", id: 342, children: []}
}
```

```elixir
neuron
|> Neuron.call([Value.new(1), Value.new(1)])
|> Mermaid.render_value()
```

<!-- livebook:{"output":true} -->

```mermaid
---
config:
  theme: base
---
flowchart LR

 
 
 
 4784[
data: 1
grad: 0.0]
 --> 4784340mul[mul]
 340[w1
data: 3.268692394832694
grad: 0.0]
 --> 4784340mul[mul]
4784340mul[mul] --> 4786[.w1
data: 3.268692394832694
grad: 0.0]

 --> 47864787sum[sum]
 
 4785[
data: 1
grad: 0.0]
 --> 4785341mul[mul]
 341[w2
data: 3.2686881147652467
grad: 0.0]
 --> 4785341mul[mul]
4785341mul[mul] --> 4787[.w2
data: 3.2686881147652467
grad: 0.0]

 --> 47864787sum[sum]
47864787sum[sum] --> 4788[x.w
data: 6.5373805095979405
grad: 0.0]

 --> 4788342add[add]
 342[b
data: -4.947583416130867
grad: 0.0]
 --> 4788342add[add]
4788342add[add] --> 4789[x.w+b
data: 1.589797093467073
grad: 0.0]

 --> 4789tanh[tanh]
4789tanh[tanh] --> 4790[output
data: 0.9201182155991248
grad: 0.0]



```

The solution this neural network has arrived at is multiply each of the inputs by 3, sum them and subtract 5. Before being passed to `tanh` this leaves `1` for `1 and 1`, `-2` for `1 and 0` and `-5` for `0 and 0`. `tanh` will the squish these values down to close to `1` and `-1` giving us approximately the correct answer.
